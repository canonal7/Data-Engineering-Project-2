{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6f3bc-ece8-4c9a-ac40-bc2ac3364c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: struct (nullable = true)\n",
      " |    |-- flight_number: string (nullable = true)\n",
      " |    |-- flight_status: integer (nullable = true)\n",
      " |    |-- planned_departure_time: string (nullable = true)\n",
      " |    |-- actual_departure_time: string (nullable = true)\n",
      "\n",
      "+-------------+\n",
      "|average_delay|\n",
      "+-------------+\n",
      "|         NULL|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|average_delay|\n",
      "+-------------+\n",
      "|         29.8|\n",
      "+-------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|29.407608695652176|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|30.033755274261605|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|30.463087248322147|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|32.137362637362635|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|31.615384615384617|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|31.643923240938165|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|30.829906542056076|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|31.339590443686006|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|30.656967840735067|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_delay|\n",
      "+-----------------+\n",
      "|30.94825174825175|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_delay|\n",
      "+-----------------+\n",
      "|30.74131274131274|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|     average_delay|\n",
      "+------------------+\n",
      "|30.823458282950423|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_delay|\n",
      "+-----------------+\n",
      "|30.28184892897407|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_delay|\n",
      "+-----------------+\n",
      "|29.96628029504742|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|average_delay|\n",
      "+-------------+\n",
      "|       29.468|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Stream_Processor\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "bucket = 'temp_bucket_prj2'\n",
    "\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"flight_number\", StringType(), True),\n",
    "     StructField(\"flight_status\", IntegerType(), True),\n",
    "     StructField(\"planned_departure_time\", StringType(), True),\n",
    "     StructField(\"actual_departure_time\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "kafkaStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"flight\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df = kafkaStream.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "df.printSchema()\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def calculate_delay(actual_departure_time, planned_departure_time):\n",
    "    delay_hours = []\n",
    "    for actual, planned in zip(actual_departure_time, planned_departure_time):\n",
    "\n",
    "        actual_time = pd.to_datetime(actual, format='%H:%M')\n",
    "        planned_time = pd.to_datetime(planned, format='%H:%M')\n",
    "        if actual_time >= planned_time:\n",
    "            delay = (actual_time - planned_time).seconds / 60\n",
    "        else:\n",
    "            delay = (24*60 - ((planned_time - actual_time).total_seconds()/60))\n",
    "        print(delay)\n",
    "        delay_hours.append(delay)\n",
    "    return pd.Series(delay_hours)\n",
    "\n",
    "processed_df = df.withColumn(\n",
    "    \"delay_time\",\n",
    "    calculate_delay(col(\"parsed_value.actual_departure_time\"), col(\"parsed_value.planned_departure_time\"))\n",
    ").select(\n",
    "    \"parsed_value.flight_number\",\n",
    "    \"parsed_value.flight_status\",\n",
    "    \"parsed_value.planned_departure_time\",\n",
    "    \"parsed_value.actual_departure_time\",\n",
    "    \"delay_time\"\n",
    ")\n",
    "# Write the processed data to BigQuery\n",
    "output_table = \"data-eng-assignment-2.assignment2.spark-data\"  \n",
    "\n",
    "# processed_df = processed_df.groupBy().count()\n",
    "\n",
    "average_delay = processed_df.selectExpr(\"avg(delay_time) as average_delay\")\n",
    "# average_delay.show()\n",
    "\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.show()\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', output_table) \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "query = average_delay.writeStream.outputMode(\"complete\") \\\n",
    "                    .trigger(processingTime = '2 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c32b1db-e96c-41bc-9d19-574f03d831e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772bdaa7-9f4b-4193-92e8-f36fb4372742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4c82b8-7e12-4105-97cb-2ee8b344e71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de8291-7b06-4f77-83bf-7179f05258c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
